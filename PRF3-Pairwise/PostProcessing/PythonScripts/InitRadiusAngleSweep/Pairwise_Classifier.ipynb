{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'enable_eager_execution'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-73115d2cb574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'enable_eager_execution'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jan 14 10:46:53 2019\n",
    "\n",
    "@author: thomas\n",
    "\"\"\"\n",
    "\n",
    "#MODULES\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image, HTML\n",
    "\n",
    "mpl.rcParams['axes.linewidth'] = 1.5 #set the value globally\n",
    "\n",
    "#CONSTANTS\n",
    "cwd_PYTHON = os.getcwd()\n",
    "PERIOD = 0.1\n",
    "DT = 1.0e-2\n",
    "RADIUSLARGE = 0.002\n",
    "RADIUSSMALL = 0.001\n",
    "\n",
    "#Lists\n",
    "#RLength\n",
    "R = [\"3\",\"5\",\"6\",\"7\"]\n",
    "SwimDirList = [\"SSL\", \"LSL\", \"Stat\"]\n",
    "\n",
    "allData = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Model Class\n",
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        #Index of class names\n",
    "        '''CHANGE'''\n",
    "        self.class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n",
    "        \n",
    "        #Setup Neural Network model\n",
    "        self.model = self.init_NN()\n",
    "        \n",
    "        #Setup the optimizer: Stochastic gradient descent (SGD)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        \n",
    "        #Setup Global Step\n",
    "        self.global_step = tf.Variable(0)\n",
    "        \n",
    "        #Setup Loss and Accuracy Results\n",
    "        self.train_loss_results = []\n",
    "        self.train_accuracy_results = []\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        return self.W*x + self.b\n",
    "    \n",
    "    def init_NN(self):\n",
    "        \"\"\"NEURAL NETWORK MODEL\"\"\"\n",
    "        #2 Hidden layers  with 10 nodes each and 1 output with 3 nodes (3 classifiers)\n",
    "        #ReLU is common for hidden layer activation\n",
    "        model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)), #input shape\n",
    "                tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "                tf.keras.layers.Dense(3)\n",
    "                ])\n",
    "        return model\n",
    "    \n",
    "    #Define a Loss Function\n",
    "    def loss(self, x, y):\n",
    "        y_ = self.model(x)\n",
    "        print(y_)\n",
    "        #Takes the model's class probability predictions and the desired label\n",
    "        #returns the average loss across the examples\n",
    "        return tf.losses.sparse_softmax_cross_entropy(labels=y,logits=y_)\n",
    "    \n",
    "    def grad(self, inputs, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            #print(tape.watched_variables())\n",
    "            loss_value = self.loss(inputs, targets)\n",
    "        return loss_value, tape.gradient(loss_value, self.model.trainable_variables)\n",
    "    \n",
    "    def train(self,train_dataset,num_epochs):\n",
    "        \"\"\"TRAINING LOOP\"\"\"\n",
    "        #1) Iterate each epoch. An epoch is one pass through the dataset\n",
    "        #2) Within an epoch, iterate over each example in the training Dataset\n",
    "        #   grabbing its features (x) and label (y)\n",
    "        #3) Using the example's features, make a prediction and compare it with the label\n",
    "        #   Measure the inaccuracy of the prediction and use it to calculate the \n",
    "        #   model's loss and gradients\n",
    "        #4) Use an optimizer to update the model's variables\n",
    "        #5) Keep track of some stats for visualization\n",
    "        #6) Repeat for each epoch\n",
    "    \n",
    "        ##Note: Rerunning this cell uses the same model variables\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss_avg = tfe.metrics.Mean()\n",
    "            epoch_accuracy = tfe.metrics.Accuracy()\n",
    "        \n",
    "            #Training loop - using batches of 32\n",
    "            for x, y in train_dataset:\n",
    "                #Optimize the model\n",
    "                loss_value, grads = self.grad(x, y)\n",
    "                print('model variables')\n",
    "                print(self.model.variables)\n",
    "                return\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.variables),self.global_step)\n",
    "                \n",
    "                #Track Progress\n",
    "                epoch_loss_avg(loss_value) #add current batch loss\n",
    "                #epoch_loss_avg = tf.metrics.mean(loss_value) #add current batch loss\n",
    "                #compare predicted label to actual label\n",
    "                epoch_accuracy(tf.argmax(self.model(x), axis=1, output_type=tf.int32),y)\n",
    "                #epoch_accuracy = tf.metrics.accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32),y)\n",
    "                \n",
    "            #end epoch\n",
    "            self.train_loss_results.append(epoch_loss_avg.result())\n",
    "            self.train_accuracy_results.append(epoch_accuracy.result())\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                print(\"Epoch {:03d}: Loss{:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                          epoch_loss_avg.result(),\n",
    "                                                                          epoch_accuracy.result()))\n",
    "        return\n",
    "    \n",
    "    def testModel(self,test_dataset):\n",
    "        #Evaluate the Model\n",
    "        #Uses only 1 epoch\n",
    "        test_accuracy = tfe.metrics.Accuracy()\n",
    "    \n",
    "        for (x,y) in test_dataset:\n",
    "            logits = self.model(x)\n",
    "            prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "            test_accuracy(prediction, y)\n",
    "        \n",
    "        print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\n",
    "    \n",
    "        tf.stack([y,prediction], axis=1)\n",
    "        return\n",
    "        \n",
    "    \n",
    "    def predict(self,predictions):\n",
    "        for i, logits in enumerate(predictions):\n",
    "            class_idx = tf.argmax(logits).numpy()\n",
    "            p = tf.nn.softmax(logits)[class_idx]\n",
    "            name = self.class_names[class_idx]\n",
    "            print(\"Example {} prediction: {} ({:4.1f}%)\".format(i, name, 100*p))\n",
    "        \n",
    "\n",
    "def pack_features_vector(features, labels):\n",
    "    \"\"\"Pack the features into a single array\"\"\"\n",
    "    features = tf.stack(list(features.values()),axis=1)\n",
    "    return features,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Download training dataset file\n",
    "    train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "    train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
    "                                               origin=train_dataset_url)\n",
    "    \n",
    "    print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))\n",
    "    \n",
    "    #column order in CSV file\n",
    "    column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
    "                    'species']\n",
    "    #features and labels\n",
    "    feature_names = column_names[:-1]\n",
    "    label_name = column_names[-1]\n",
    "    \n",
    "    print('Features: {}'.format(feature_names))\n",
    "    print('Label: {}'.format(label_name))\n",
    "    \n",
    "    \"\"\"TRAINING DATA\"\"\"\n",
    "    #Create a tf.data.Dataset for training\n",
    "    batch_size = 32\n",
    "    train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "            train_dataset_fp,\n",
    "            batch_size,\n",
    "            column_names=column_names,\n",
    "            label_name=label_name,\n",
    "            num_epochs=1)\n",
    "    \n",
    "    features, labels = next(iter(train_dataset))\n",
    "    \n",
    "    print('Label: {}'.format(labels))\n",
    "    \n",
    "    #Use Seaborn to plot all pairwise plots\n",
    "    #dataset = pd.read_csv(train_dataset_url, header=None, names=['sepal_length','sepal_width','petal_length','petal_width','species'])\n",
    "    #print(dataset[1:])\n",
    "    #seaborn.pairplot(dataset[1:], hue=\"species\", size=2, diag_kind=\"kde\")\n",
    "    #plt.show()\n",
    "    #return\n",
    "    \n",
    "    #Plot a few features from the batch\n",
    "    fig = plt.figure(1)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(features['petal_length'],\n",
    "               features['sepal_length'],)\n",
    "               #c=labels,\n",
    "               #cmap='viridis')\n",
    "    ax.set_xlabel('Petal length')\n",
    "    ax.set_ylabel('Sepal length')\n",
    "    \n",
    "    train_dataset = train_dataset.map(pack_features_vector)\n",
    "    #features element of dataset are now arrays with shape (batch_size, num_features)\n",
    "    features, labels = next(iter(train_dataset))\n",
    "    print(features[:5])\n",
    "    \n",
    "    \"\"\"Create Model for Classification Learning\"\"\"\n",
    "    model1 = Model()\n",
    "    #The more hidden layers, the more powerful. BUT need more data in return\n",
    "    #Predictions before Model is used\n",
    "    predictions = model1.model(features)\n",
    "    #print(model1.model.variables)\n",
    "    #print non-normalized probability values\n",
    "    print(predictions[:5])\n",
    "    #print normalized probability values using softmax\n",
    "    print(tf.nn.softmax(predictions[:5]))\n",
    "    #print prediction using argmax\n",
    "    #Note: should not be good predictions\n",
    "    print(\"Prediction: {}\".format(tf.argmax(predictions,axis=1)))\n",
    "    print(\"    Labels: {}\".format(labels))\n",
    "    \n",
    "    \"\"\"TRAINING THE MODEL!!!\"\"\"\n",
    "    l = model1.loss(features, labels)\n",
    "    print(\"Loss Test: {}\".format(l))\n",
    "    #Calculate gradient tapes used to optimize our model\n",
    "    #Calculate a single optimization step\n",
    "    loss_value, grads = model1.grad(features, labels)\n",
    "    print(\"Step: {}, Initial Loss: {}\".format(model1.global_step.numpy(),\n",
    "                                              loss_value.numpy()))\n",
    "    model1.optimizer.apply_gradients(zip(grads, model1.model.variables),model1.global_step)\n",
    "    print(\"Step: {},         Loss: {}\".format(model1.global_step.numpy(),\n",
    "                                              model1.loss(features,labels).numpy()))\n",
    "    \n",
    "    \"\"\"TRAINING\"\"\"\n",
    "    #Tunable variables\n",
    "    num_epochs = 201\n",
    "    model1.train(train_dataset, num_epochs)\n",
    "    \n",
    "    #Visualize Loss Function over Time\n",
    "    fig, axes = plt.subplots(2, sharex=True, figsize=(12,8))\n",
    "    fig.suptitle('Training Metrics')\n",
    "        \n",
    "    axes[0].set_ylabel(\"Loss\",fontsize=14)\n",
    "    axes[0].plot(model1.train_loss_results)\n",
    "        \n",
    "    axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "    axes[1].set_xlabel(\"Epoch\", fontsize = 14)\n",
    "    axes[1].plot(model1.train_accuracy_results)\n",
    "        \n",
    "    \"\"\"TEST THE MODEL!!!\"\"\"\n",
    "    #Setup test dataset\n",
    "    test_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "    test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),\n",
    "                                      origin=test_url)\n",
    "    \n",
    "    test_dataset = tf.contrib.data.make_csv_dataset(\n",
    "            test_fp,\n",
    "            batch_size,\n",
    "            column_names=column_names,\n",
    "            label_name='species',\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "    \n",
    "    test_dataset = test_dataset.map(pack_features_vector)\n",
    "    \n",
    "    #Evaluate the Model\n",
    "    model1.testModel(test_dataset)\n",
    "    \n",
    "    #Make Predictions!\n",
    "    predict_dataset = tf.convert_to_tensor([\n",
    "            [5.1, 3.3, 1.7, 0.5],\n",
    "            [5.9, 3.0, 4.2, 1.5],\n",
    "            [6.9, 3.1, 5.4, 2.1]\n",
    "            ])\n",
    "    predictions = model1.model(predict_dataset)\n",
    "    model1.predict(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
